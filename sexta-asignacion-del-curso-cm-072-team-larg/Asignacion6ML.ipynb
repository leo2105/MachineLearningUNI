{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asignación 6\n",
    "\n",
    "Todos los asignaciones serán presentadas utilizando los cuadernos de [Jupyter Notebook](http://jupyter.org/),  además de respectivas pruebas como ejemplo, así como el uso de mediciones de velocidad de ejecución utilizando el comando `timeit` y algunos gráficos si fuese necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lista de preguntas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 .Presenta un reporte relacionado al clustering del artículo de Kevin Poulsen: [How a Math Genius Hacked OkCupid to Find True Love](https://www.wired.com/2014/01/how-to-hack-okcupid/all/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tu reporte\n",
    "\n",
    "El artículo describe como Chris McKinlay, un matemático que se encontraba haciendo su tesis de doctorado, utilizó un algoritmo de agrupación para comprender como funciona el sitio de citas OkCupid para poder mejorar su propio perfil, esta página hace uso de datos catégoricos a la hora de recolectar las respuestas de las preguntas en el sitio. Para ello McKinlay utilizó un algoritmo llamado K-modos que funciona directamente en este tipo de datos. \n",
    "\n",
    "Para ello tuvo que crear varias cuentas para poder recolectar el banco de preguntas del sitio, donde cada una de estas preguntas tiene una puntuación definida del 1 al 5. El algoritmo de esta página hace uso de las preguntas que contesten para así poder hallar una pareja \"compatible\". Por ello se tenía que responder las preguntas que de verdad interesaban a las mujeres para así poder elevar las probabilidades de encontrar pareja. \n",
    "\n",
    "Luego de obtener dichas preguntas, hizo uso de esta data para agrupar a las mujeres en base a las preguntas que ellas consideraban importantes, y además hallar similitudes entre estas preguntas, para ello dicidio usar el algoritmo de K-modes para poder hallar el patrón que relacionaba a estas preguntas.\n",
    "\n",
    "\n",
    "### Algoritmo K-modes\n",
    "\n",
    "La técnica de agrupación K-means no puede agrupar datos categóricos debido a las diferentes medidas que utiliza. Los algoritmos de agrupamiento como K-modes se basan en el patrón K-mean además de eliminar la limitación de datos numéricos, incluso a pesar de preservar su efectividad.\n",
    "\n",
    "Esta técnica de K-modes mejora el patrón K-mean a los datos categóricos del grupo mediante la eliminación de la limitación forzada por K-means haciendo las siguientes modificaciones:\n",
    "\n",
    "* Utilizando una coincidencia simple diferente Evalúe o anule la distancia utilizada para el objeto de datos categóricos\n",
    "* Cambiar la media de agrupación por la moda.\n",
    "\n",
    "$$\n",
    "d(x,y) = \\sum^f_{i=1} \\delta X_j Y_j \n",
    "$$\n",
    "\n",
    "Donde $d (x, y)$ otorga la misma importancia a todo tipo de atributo.\n",
    "Haciendo que Z sea un conjunto de objetos de datos categóricos descritos por los atributos categóricos $A_1, A_2, \\cdots$ mientras que la ecuación anterior se usa debido a la diferenciación determinada para los objetos de datos categóricos, la función de costo se vuelve:\n",
    "\n",
    "$$\n",
    "C(Q) = \\sum^n_{i=1} d(Z_i, Q_i) \n",
    "$$\n",
    "\n",
    "Donde $Z_i$ es el i-ésimo elemento y $Q_i$ es el grupo más cercano al centroide de $Z_i$. La técnica K-modes minimiza la función de costo definida anteriormente.\n",
    "\n",
    "Los k-modes asume que la información del numero de grupos probables de datos como $K$ son accessibles y consiste de los siguientes pasos:\n",
    "\n",
    "1. Generar K grupos arbitrariamente selecionando los objetos de datos y escogiendo un centroide de K grupo inicial, uno por cada grupo.\n",
    "2. Asignar la data del objeto al grupo cuyo centroide sea el más cercano de acuerdo a la segunda ecuación.\n",
    "3. Actualizar el valor del grupo base agrupando los objetos de la data y calculando la moda de cada grupo.\n",
    "4. Repetir el paso 2 y 3 hasta que ningun objeto de dato cambie la relación del grupo, en caso contrario añadir un nuevo criterio predefinido hasta que termine. [paper](https://pdfs.semanticscholar.org/1069/2c9b80be922903526682f8fae5ad6ffb68f6.pdf)\n",
    "\n",
    "Para ello previamente se debe convertir cada uno de los cuestionarios llenados en un punto de datos en un espacio de 40 dimensiones de la siguiente manera: Las primeras cuatro dimensiones / características registrarán la respuesta a la primera pregunta. Si respondieron (A) a la primera pregunta, los primeros cuatro valores serían  (1,0,0,0) . Si respondieron (B), sería  (0,1,0,0) y así sucesivamente. De manera similar, las siguientes cuatro características registrarían la respuesta a la segunda pregunta de la misma manera, y así sucesivamente, como en la figura de abajo. Entonces cada cuestionario nos daría un vector de 40 dimensiones con diez 1s y el resto ubicaría todos los 0s.\n",
    "\n",
    "![](https://shapeofdata.files.wordpress.com/2014/03/surveydata1.png)\n",
    "\n",
    "\n",
    "Otro motivo por el cúal se uso este algoritmo era debido a que el algoritmo K-modes simplemente registra qué respuesta a cada pregunta obtuvo la mayor cantidad de votos. Este es la moda de las respuestas, la respuesta más común, que es de donde proviene el nombre  K-modes. De modo que cada centroide está en la misma forma que los datos originales del cuestionario, un conjunto de respuestas a las diferentes preguntas, en lugar de un vector de 40 dimensiones.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 .Presenta un reporte del video relacionado al libro `An Introduction to Statistical Learning` de Gareth James, Daniela Witten, Trevor Hastie y Robert Tibshirani sobre [Clasificación jerárquica](https://www.youtube.com/watch?v=Tuuc9Y06tAc&list=PL5-da3qGB5IBC-MneTc9oBZz0C6kNJ-f2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tu reporte\n",
    "\n",
    "### Clustering \n",
    "\n",
    "* La característica de k-means es que requiere que especifiques el número de cluster por lo que se necesita tener una estrategia para poder elegir adecuadamente el número de clusters $K$.\n",
    "\n",
    "* Por otro lado Clustering jerárquico no requiere especificar algun $K$.\n",
    "\n",
    "* Clustering aglomerativo es un tipo de clustering jerárquico, y se refiere al hecho que un dendograma es construido empezando por las hojas.\n",
    "\n",
    "Para los algoritmos de clustering jerárquico se sigue lo sgte. :\n",
    "* Empezar con cada punto en su propio cluster.\n",
    "* Identificar los cluster mas cercanos y combinarlos.\n",
    "* Repetir.\n",
    "* Terminar cuando todos los puntos estan en un simple cluster.\n",
    "\n",
    "![jerarquico](images/clusters.PNG)\n",
    "\n",
    "Esto puedo ser representado como un dendograma: \n",
    "\n",
    "![dendograma](images/dendograma.PNG)\n",
    "\n",
    "\n",
    "Los clusters armados dependera de donde cortemos el dendograma como muestra la siguiente figura: \n",
    "\n",
    "![image.png](images/appClustering.PNG)\n",
    "\n",
    "En el primer caso se obtiene el dendograma con complete linkage y distnacia euclidiana. La segunda se corto el dendograma 9 de altura teniendo como resultado 2 clusters. El último se obtiene cortando a una altura de 5 obteniendo 3 clusters.\n",
    "\n",
    "### Tipos de Enlace (Linkage)\n",
    "\n",
    "* **Completo** : Registra el mas largo de las disimilaridades entre clusters.\n",
    "* **Simple ** : Registra en mínimo de las disimilaridades entre clusters.\n",
    "* **Promedio** : Registra el promedio de las disimilaridades entre clusters.\n",
    "* **Centroide** : Registra disimilaridades entre los centroides entre clusters. \n",
    "\n",
    "Los cuales los más usados son Enlace Completo, Enlace Centroide.\n",
    "\n",
    "\n",
    "### Eleccion de medida de disimilaridad\n",
    "\n",
    "* Una alternativa a la clasica distancia Euclidiana es la distancia basada en correlacion que considera 2 observaciones similares si sus caracteristicas son altamente correlacionadas.\n",
    "\n",
    "\n",
    "### Algunos problemas comunes\n",
    "\n",
    "* En el caso de clustering jerárquico Que medida de disimilaridad deberiamos usar ? Qué tipo de Enlace deberiamos de usar ?\n",
    "\n",
    "* Las observaciones deben de ser estandarizadas ? \n",
    "\n",
    "* Cuantos cluster deberiamos elegir ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 .Lee el excelente artículo de cott Fortmann-Roe [Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)  responde las siguientes preguntas mientras lees:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* En el ejemplo ` Party Registration`, ¿cuáles son las características? ¿Cuál es la respuesta? ¿Es esto un problema de regresión o clasificación?.\n",
    "\n",
    "Las características del ejemplo ` Party Registration` son que cada votante del dataset esta etiquetado con 3 propiedades: Registro de votante,  riqueza  del votante y medida cuantitativa de la religiosidad del votante. La respuesta es la prediccion de regisstro de votantes usando la riqueza del votante y la medida cuantitativa de la religiosdad del votante.\n",
    "\n",
    "Este tipo de problemas se considera como una regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conceptualmente, ¿cómo se aplica KNN a este problema para hacer una predicción?.\n",
    "\n",
    "Como es un problema de datos binarios, usualmente se usan regresiones logísticas, pero sien embargo como es un problema que no sabemos sihay no linearidad en la relacion entre las variables se usa KNN.\n",
    "\n",
    "En KNN, el registro de un votante sera encontrado ploteandolo en el plano con los otros votantes. Los otros k votantes mas cercanos a el serán hallados usando una medida geografica de distancia y el promedio de los registros seran usados para predecir su registro del votante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ¿Cómo se relacionan las cuatro visualizaciones en la sección 3 entre sí? Cambia el valor de K utilizando el slider y explica  qué cambió en las visualizaciones (y por qué cambió).\n",
    "\n",
    "Los votantes Republicanos son en su mayoria votantes con alta riqueza y alta medida cuantitativa de religiosidad.\n",
    "\n",
    "Conforme aumenta el numero de K usando el slider aumenta el número de regiones distintas a ser identificadas. Al cambiar el k con el slider, se actualizará el ploteo para mostrar como la prediccion cambia cuando k cambia.\n",
    "\n",
    "Conforme aumenta el valor de K se tienen colores mas claros, es decir con menor precisión de las predicciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* En las figuras 4 y 5, ¿qué significan los colores más claros en comparación con los colores más oscuros? ¿Cómo se calcula estos colores más oscuros?.\n",
    "\n",
    "Los colores claros indican principalmente menos precisión acerca de las predicciones.  Mientras mas oscura sea el color mayor será la precisión en las predicciones. \n",
    "\n",
    "Se tiene el error de la siguiente forma : \n",
    "\n",
    "$Err(x) = Bias^{2}+Variance + Irreducible Error$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ¿Qué representa la línea negra en la figura 5? ¿Qué predicciones haría el mejor modelo del machine learning con respecto a esta línea?.\n",
    "\n",
    "La línea limite negra indica el mejor valor de predicción . Si las predicciones coinciden con él, tendrán mayor precisión. Además es la línea que limita entre los democráticos y Republicanos según la predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Escoge un valor muy pequeño de K y haga clic en el botón `Generate New Training Data` varias veces. ¿Ves `baja varianza` o `alta varianza`, `bajo sesgo` o `alto sesgo`?.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Repite esto con un valor muy grande de K. ¿Ves `baja varianza` o `alta varianza`,  `bajo sesgo` o `alto sesgo`?."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Intenta usar otros valores de K. ¿Qué valor de K crees que es `mejor`? ¿Cómo defines ` mejor`?.\n",
    "\n",
    "Mejor es aquel valor de K que produce mayor precisión en la predicción. Siguiendo esta definicióna menor $K$ se tiene mayor precisión debido a que los colores oscuros representan alta precisión en el algoritmo y los colores claros representan una baja precisión en la predicción ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ¿Un pequeño valor para K causa `overfitting` o `underfitting`?."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ¿Por qué debería importarnos la varianza? ¿No deberíamos minimizar el sesgo e ignorar la varianza?."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 . Presenta un reporte del artículo Andrew Ng:  [On Discriminative vs. Generative classifier: A comparison of logistic regression an naive Bayes](http://ai.stanford.edu/%7Eang/papers/nips01-discriminativegenerative.pdf) que compara el rendimiento de la regresión logística y Naive Bayes en una variedad de conjuntos de datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Andrew Ng: On Discriminative vs Generative classifier: A comparison of logistic regression an Naive Bayes\n",
    "\n",
    "En el articulo presentan una comparacion entre aprendizaje discriminativo y generativo conocidos como regresion logistica y Naive Bayes. Se muestra principalmente que contrario a lo que creen que clasificadores discriminativos son casi siempre preferidos, hay frecuentemente 2 distintos regimenes de rendimiento como el tamaño del conjunto de entrenamiento se incrementa.\n",
    "\n",
    "Los resultados de los experimentos del artículo fueron que incluso aunque el algoritmo de regresión logistico discriminativo tiene menor error asintotico, el clasificador generative naive Bayes podría tambien converger mas rapidamente para su error asintotico.\n",
    "\n",
    "Para sus experimentos usaron 15 Dataset, 8 con variables continuas, 7 con discrtas, del repositorio UCI Machine Learning \n",
    "El articulo concluye que el primero fue asintoticamente muy leve menos estadisticamente eficiente. Una segunda importante diferencia es que Efron es consideradosolo un caso especial en que el $P(x|y)$ es Gausiano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 . Considera el problema de aprendizaje de la regresión logística: Sea $H = X = \\{x \\in R^{d}: \\Vert x \\Vert \\leq B\\}$ para algún escalar $B > 0$, sea $ Y = \\{\\pm 1 \\}$ y sea la función de pérdida $l$ definida como\n",
    "\n",
    "$$\n",
    "l(w, (x, y)) = \\log(1 + \\exp(−y\\langle w, x\\rangle))\n",
    "$$\n",
    "\n",
    "Demuestra que el problema de aprendizaje resultante es convexo, Lispchitz  y acotado y regular. Especifica los parámetros de Lipschitz y de regularidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tu solucion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 .El método KNN admite  distancia métricas distintas de la distancia euclidiana, como la distancia Mahalanobis, que tiene en cuenta la escala de los datos. Presenta un reporte de los siguientes notas, con la finalidad de que se obtenga un entendimiento de lo que KNN significa.\n",
    "\n",
    "* [Distance metrics in KNN](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html).\n",
    "* [Explanation of the Mahalanobis distance](https://stats.stackexchange.com/questions/62092/bottom-to-top-explanation-of-the-mahalanobis-distance).\n",
    "* [Mahalanobis distance and scale of the data into account](https://blogs.sas.com/content/iml/2012/02/15/what-is-mahalanobis-distance.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tu reporte\n",
    "\n",
    "El algoritmo KNN trabaja sobre una metrica llamada distancia la cual permite definir un separador para la data.\n",
    "\n",
    "\n",
    "Entre las más comunes tenemos a la distancia Euclidiana ($\\sqrt{sum((x - y)^2)}$), Manhattan ($sum(\\|x - y\\|)$), Chebyshev ($max(\\|x - y\\|$)).\n",
    "\n",
    "Entre otras tenemos a la Mahalanobis la cual esta definida asi:\n",
    "\n",
    "$$D = \\sqrt{(x-\\mu)^{T} V^{-1}(x-\\mu)}$$\n",
    "\n",
    "x : Vector de data, \n",
    "\n",
    "$\\mu$ : Vector de medias\n",
    "\n",
    "V : matriz de Covarianza de variables independientes\n",
    "\n",
    "\n",
    "Esta medida para ser llamada distancia debe cumplir las siguientes propiedades:\n",
    "\n",
    "1. No negatividad: d(x,y) >= 0\n",
    "2. Identidad: d(x,y) = 0 iff x == y\n",
    "3. Simetria: d(x,y) = d(y,x)\n",
    "4. Desigualdad Triangular: d(x,y) + d(y,z) >= d(x,z)\n",
    "\n",
    "Las distancias de Mahalanobis nos proporciona un metodo poderoso para medir cuan similares son un conjunto de condiciones para un conjunto ideal de condiciones, y puede ser muy utiles para identificar que regiones del paisajes son más similares a un paisaje \"ideal\" en términos de reconocimiento de patrones.\n",
    "Por ejemplo, en el campo de la biología de la vida silvestre podríamos definir un paisaje \"ideal\" como el que mejor se ajusta al nicho de algunas especies de vida silvestre. A través de la observación, podemos encontrar que una especie de vida silvestre típicamente ocurre dentro de un rango de elevación particular, en pendientes de una inclinación particular, y tal vez dentro de cierta densidad de vegetación. Usando distancias de Mahalanobis, podemos describir cuantitativamente todo el paisaje en términos de su similitud con la elevación ideal, la pendiente y la densidad de vegetación de ese animal.\n",
    "\n",
    "En el caso de una data normalmente distribuida univariada, cuando hablamos en términos de \"proximidad\", preferiríamos que la data estuviera a lo mucho una unidad de desviación estándar, ya que la función de densidad es mucho más alta cerca a la media cero que cuando te mueves muchas desviaciones estandar lejos.\n",
    "\n",
    "Tambien, podemos especificar la distancia calculando el *z-score*. $z = (x-\\mu) / \\sigma$.\n",
    "\n",
    "Esta idea se puede generalizar para data multivariada. El siguiente grafico nos muestra una data normal bivariada que es sobrelapada con elipses de prediccion. Los elipses representan 10%(cercano), 20%, ..., 90%(lejano) elipse de prediccion para la distribucion normal bivariada que generó los datos.\n",
    "\n",
    "\n",
    "![ols](images/mahal.png)\n",
    "\n",
    "\n",
    "En el grafico, vemos dos observaciones representadas por las marcas de color rojo. la primera en la coordena (4,0) y la segunda (0,2). La pregunta vendría a ser, ¿Cuál marca está más cerca al origen?\n",
    "\n",
    "Depende con que métrica las midamos. Si fuera la distancia euclidiana, diríamos que el punto (0,2) está más cerca al origen. Sin embargo, para esta distribución, la varianza en el eje Y es mayor que la varianza en la dirección X. Entonces, vemos que de cierta manera el punto (0,2) esta \"mas desviaciones estandar\" lejos que el origen que el punto (4,0).\n",
    "\n",
    "Además, notamos que el punto (0,2) esta localizado en el 90% del elipse de predicción, en cambio, el punto (4,0) esta en el 75%. Esto significa que el punto (4,0) se encuentra más \"cerca\" al origen que el punto (0,2). La funcion de probabilidad de densidad es mayor cerca a (4,0) que a (0,2).\n",
    "\n",
    "\n",
    "Si quisiera entenderlo de una manera más \"sencilla\" podemos realizar la transformación de Cholesky de variables correlacionas a no correlacionadas:\n",
    "\n",
    "![hi](images/hola3.png)\n",
    "\n",
    "donde el eje azul vendría  a ser el eje principal o eje X y el eje rojo vendría a ser el eje secundario o eje Y. Cada unidad en cada eje se considera como desviación estándar.\n",
    "\n",
    "Se puede usar los contornos de probabilidad para definir la distancia de Mahalanobis. Esta distancia tiene las siguientes propiedades:\n",
    "\n",
    "1. La varianza en cada dirección es diferente.\n",
    "2. Considerar la covarianza entre las variables.\n",
    "3. Se reduce a la distancia euclidiana familiar para las variables no correlacionadas con la varianza unitaria.\n",
    "\n",
    "Para los datos normales univariados, el z-score univariante estandariza la distribución (de modo que tiene una media 0 y la unidad de varianza) y proporciona una cantidad adimensional que especifica la distancia desde un punto de observación a la media en términos de la escala de los datos. Para datos normales multivariados con media $\\mu$ y matriz de covarianza $\\Sigma$, puede descorrelacionar las variables y estandarizar la distribución aplicando la transformación de Cholesky $z = L^{-1} (x - \\mu)$, donde L es el factor de Cholesky de $\\Sigma, \\Sigma = LL^{T}$.\n",
    "\n",
    "Después de transformar la data, se puede calcular la distancia Euclidiana estándar desde el punto z al origen. Para deshacerse de las raíces cuadradas, vamos a calcular el cuadrado de la distancia Euclidiana, el cual es $dist^{2}(z,0) = z^{T}z $. Esto mide que tán lejos esta origen al punto, y esto es la generalización multivariado del z-score.\n",
    "\n",
    "Se puede reescribir $zz^{T}$ en términos de las variables correlacionadas originales. La distancia cuadrada $Mahal^{2}(x,\\mu)$ es\n",
    "\n",
    "$$ = z^{T}z \n",
    "= (L^{-1} (x - \\mu))^{T} (L^{-1}(x - \\mu)) \n",
    "= (x - \\mu)^{T} (LL^{T})^{-1} (x - \\mu) = (x - \\mu)^{T} (LL^{T})^{-1} (x - \\mu) = (x - \\mu)^{T} \\Sigma^{-1} (x-\\mu)$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 .(Reto) Demuestre que el modelo de densidad KNN define una distribución incorrecta   integral en todo el espacio es divergente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 . Demuestra que para Naive Bayes multicategoría la decisión óptima está dada por:\n",
    "\n",
    "$$\n",
    "y^{*}(x) = \\text{argmax}_y p(y)\\prod_{i =1}^{n}p([x]_iy)\n",
    "$$\n",
    "\n",
    "donde $y \\in Y$ es la etiqueta de clase de la observación $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las referencias a los ejercicios son :\n",
    "\n",
    "   * David Barber, Bayesian Reasoning and Machine Learning, Cambridge University Press, 2017.\n",
    "   * Pattern Recognition and Machine Learning de Chris Bishop 2006.\n",
    "   * Shai Shalev-Shwartz, and Shai Ben-David,Understanding Machine Learning: From Theory to Algorithms Cambridge University Press, 2014. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
