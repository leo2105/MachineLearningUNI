{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asignación 4\n",
    "\n",
    "Todos los asignaciones serán presentadas utilizando los cuadernos de [Jupyter Notebook](http://jupyter.org/),  además de respectivas pruebas como ejemplo, así como el uso de mediciones de velocidad de ejecución utilizando el comando `timeit` y algunos gráficos si fuese necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lista de preguntas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 . Considera un conjunto de entrenamiento que es linealmente separable con un margen $\\gamma $ y tal que todas las instancias están dentro de una bola de radio $\\rho$. Demuestra que el número máximo de actualizaciones que realizará el algoritmo de `Batch Perceptron` cuando se ejecuta en este conjunto de entrenamiento es $(\\rho/\\gamma)^2$.\n",
    "\n",
    "Sugerencia: Revisa el capítulo 9  del libro Shai Shalev-Schwartz para conocer más acerca del algoritmo de `Batch Perceptron`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tu respuesta\n",
    "\n",
    "Dado un conjunto de m elementos $(x_1, y_1),..., (x_m,y_m)$ separable, tenemos un w\\* que alcanza la minima norma para la ecuacion $y_i\\langle$ w\\*$, x_i\\rangle \\geq 1$ para todo i en 1..m .\n",
    "\n",
    "Como vemos en el Batch Perceptron, $w^{(1)}$ = (0,...,0) y $\\langle$ w\\*$, w^{(1)}\\rangle = 0$, mientras que para t, si acualizamos usando el ejemplo $(x_i, y_i)$ vamos a tener que:\n",
    "\n",
    "\n",
    "$$\\langle w^\\ast,w^{(t+1)}\\rangle - \\langle w^\\ast,w^{(t)}\\rangle = \\langle w^\\ast,w^{(t+1)} - w^{(t)} \\rangle = \\langle w^\\ast , y_{i}x_{i}\\rangle(act. Perceptron) = y_{i}\\langle w^\\ast,x_{i} \\rangle \\geq 1$$\n",
    "\n",
    "Despues de T iteraciones, conseguimos:\n",
    "\n",
    "$$\\langle w^\\ast, w^{(T+1)}\\rangle = \\sum^{T}_{t=1} \\big( \\langle w^\\ast, w^{(t+1)}\\rangle - \\langle w^\\ast, w^{(t)}\\rangle \\big) \\geq T$$\n",
    "\n",
    "Luego, encontrando un limite superior para $\\|w^{(T+1)}\\|$. Para cada iteracion $t$ tenemos que:\n",
    "\n",
    "$$ \\| w^{(t+1)} \\|^2 = \\| w^{(t)} + y_ix_i\\|^2$$ \n",
    "\n",
    "$$ = \\|w^{(t)}\\|^2 + 2y_i\\langle w^{(t)},x_i\\rangle + y_i^2 \\|x_i\\|^2 $$\n",
    "\n",
    "\n",
    "$$\\leq \\|w^{(t)}\\|^2 + R^2$$  \n",
    "\n",
    "De la ultima desigualdad podemos obtener el caso que $y_i \\langle w^{(t)}, x_i\\rangle  \\leq 0$, y la norma de x_i a lo mas R. Esta ultima ecuacion la realizamos recursivamos T veces y consideramos que $\\| w^{(1)} \\|^2 = 0$,  obtendremos:\n",
    "\n",
    "$$\\|w^{(T+1)}\\|^2 \\leq TR^2 \\Rightarrow \\|w^{(T+1)}\\| \\leq \\sqrt{T}R $$\n",
    "\n",
    "Luego combinando ecuaciones anteriores obtenemos:\n",
    "\n",
    "$$\\frac{\\langle w^{(T+1)},w^\\ast \\rangle}{\\|w^\\ast\\| \\|w^{(T+1)}\\|} \\geq \\frac{T}{B\\sqrt{T}R} = \\frac{\\sqrt{T}}{BR}$$\n",
    "\n",
    "Aplicando desigualdad de Cauchy-Schwartz obtenemos,\n",
    "\n",
    "$$1 \\geq \\frac{\\sqrt{T}}{BR} \\Rightarrow T \\leq (RB)^2$$\n",
    "\n",
    "Tenemos el margen igual a $\\gamma = \\frac{1}{B}$, donde este margen se obtiene al minimizar el $\\|w\\|$ (el cual vendria ser igual a $\\|w^\\ast\\|$)  y la bola de radio $\\rho$ = R ($max_i(\\|x_i\\|$)) esto es asumiendo que para cada dato $x_i$ tiene norma euclidiana.\n",
    "\n",
    "Concluimos que:\n",
    "\n",
    "$$T \\leq \\Big(\\frac{\\rho}{\\gamma}\\Big)^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 . Demuestre o refuta la siguiente propiedad :\n",
    "\n",
    "Existe $\\lambda > 0$ tal que para cada muestra S de $m > 1$ ejemplos, que es separable por la clase de semiespacios  homogéneos, las reglas de aprendizaje del  SVM-fuerte y SVM-suave (con parámetro $\\lambda$) devuelven exactamente el mismo vector de peso.\n",
    "\n",
    "Sugerencia: Refuerza las lecturas de las clase con la presentación de Tom Mitchell: [Support Vector Machines-Kernel Methods](https://www.cs.cmu.edu/~tom/10701_sp11/recitations/rec12.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 . Muestra  cómo ejecutar el algoritmo de Perceptron mientras solo se accede a las instancias a través de una función de kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 . El problema de  regresión Ridge, con un mapeo de características $\\psi$, es el problema de encontrar un vector $w$ que minimice la función:\n",
    "\n",
    "$$\n",
    "f(w) = \\lambda \\Vert w\\Vert^2 + \\frac{1}{2m}\\sum_{i=1}^{m}(\\langle w, \\psi(x_i)\\rangle -y_i)^2\n",
    "$$\n",
    "\n",
    "y entonces retornando el predictor:\n",
    "\n",
    "$$\n",
    "h(x) = \\langle w, x \\rangle.\n",
    "$$\n",
    "\n",
    "Muestra como implementar la regresión Ridge con Kernels.\n",
    "\n",
    "Sugerencia:  Asume que existe un $\\alpha \\in \\mathbb{R}^m$ tal que $\\sum_{i =1}^{m}\\alpha_i\\psi(x_i)$ es el minimizador de la ecuación anterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 . A un gerente de supermercado le gustaría aprender  cuáles de sus clientes tienen bebés sobre la base de sus carritos de compra. Específicamente, el geremte muestreó  clientes idénticamente distribuidos independientes, donde para el cliente $i$, sea $x i \\subset \\{1,\\dots , d\\}$ denota el subconjunto de artículos que el cliente compró y sea $y_i \\in \\{\\pm 1\\}$  la etiqueta que indica si este cliente tiene un bebé. \n",
    "\n",
    "Como conocimiento previo, el gerente sabe que hay $k$ elementos tales que se determina que la etiqueta es $1$ si el cliente compró al menos uno de esos $k$ artículos. De esta manera, se desconoce la identidad de estos elementos $k$.\n",
    "\n",
    "Además, de acuerdo con la regulación de la tienda, cada cliente puede comprar como máximo $s$ artículos. Ayuda al gerente a diseñar un algoritmo de aprendizaje de modo que tanto su complejidad de tiempo como su complejidad de muestra sean polinomiales en $s$, $k$ y $1/\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 .Sea $N$ un número entero positivo. Para cada $x, x^{'} \\in \\{1, \\dots, N \\}$ definimos\n",
    "\n",
    "$$\n",
    "K(x, x^{'}) = \\min \\{x, x^{'}\\}.\n",
    "$$\n",
    "\n",
    "Prueba que $K$ es un kernel válido, a saber, encuentra una función $\\psi: \\{1,\\dots, N\\} \\rightarrow H$ donde $H$ es un espacio de Hilbert, tal que\n",
    "\n",
    "$$\n",
    "\\forall x, x^{'} \\in \\{1,\\dots, N\\},\\ K(x,x^{'}) = \\langle \\psi(x), \\psi(x^{'})\\rangle.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tu respuesta\n",
    "El espacio completo con el producto interno tambíen es llamado espacio de Hilbert y en este caso es $H$.\n",
    "\n",
    "Nos piden probar que $K$ es un kernel válido. Para ello la función debe cumplir 2 condiciones : \n",
    "\n",
    "a) **Simetría : $K(x, x^{'}) = K( x^{'},x)$**\n",
    "\n",
    "b) **positive semi-definiteness **\n",
    "\n",
    "\n",
    "Evaluamos la primera condición :\n",
    "\n",
    "a) \n",
    "\n",
    "\n",
    "$K(x, x^{'}) = \\min \\{x, x^{'}\\} = \\min \\{ x^{'},x\\} = K( x^{'},x) $\n",
    "\n",
    "$-> K(x, x^{'}) = K( x^{'},x)$\n",
    "\n",
    "\n",
    "b)\n",
    "\n",
    "\n",
    " Consiste en encontrar una función $\\phi(x)$ tal que $K(x, x^{'}) $ =  $\\phi(x)^{T}\\phi(x^{'})$ O Tambiíen satisfacer esta desigualdad mediante el teorema de Mercer:\n",
    " \n",
    " \n",
    " $\\sum_{i}^{n}\\sum_{j=1}^{n}c_{i}c_{j}K(x_{i},x_{j}) \\geq 0$\n",
    " \n",
    " =  $\\sum_{i=1}^{n}\\sum_{j=1}^{n}c_{i}c_{j}K(x_{i},x{j}) $\n",
    " = $\\sum_{i=1}^{n}\\sum_{j=1}^{n}c_{i}c_{j} \\min \\{ x_{i},x_{j}\\} $ \n",
    " \n",
    " Como sabemos de dato que $x_{i}, x_{j}\\in \\{1, \\dots, N \\}$ \n",
    " $ \\min \\{ x_{i},x_{j}\\} \\geq 0$ Sin embargo no nos asegura de que para cualquier c_{i}, c_{j} se tenga lo siguiente :\n",
    " \n",
    "  $  -> \\sum_{i=1}^{n}\\sum_{j=1}^{n}c_{i}c_{j}K(x_{i},x_{j}) \\geq 0$\n",
    " \n",
    "\n",
    "Hacemos un análisis de positive definite-ness por una simulación aleatoria hecha en python y analizar el eigenvalor de la matriz por teorema de Mercer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1.06756535057e-11+0j)\n",
      "(-5.46118918004e-13+0j)\n",
      "(-1.01544102164e-12+0j)\n",
      "(-4.05879873342e-15-2.39632848603e-15j)\n",
      "(-1.45755397436e-11+0j)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg as LA\n",
    "\n",
    "# Variables para análisis de simulación aleatoria.\n",
    "a=1\n",
    "b=1000\n",
    "N=1000\n",
    "\n",
    "n_simul = 5\n",
    "\n",
    "for i in range(n_simul):\n",
    "    X = np.random.randint(a, b, N)\n",
    "    K=  np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            K[i,j]=min(X[i],X[j])\n",
    "\n",
    "    # Analizamos el menor de los eigenvalores\n",
    "    print(min(LA.eigvals(K)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como tenemos eigenvalores negativos en la simulación podemos decir que la función $K(x, x^{'}) = \\min \\{ x,x^{'}\\}  $ no es un Kernel válido.\n",
    "\n",
    "\n",
    "En contraste la función $K(a, b) = \\sum_{i=1}^{n} \\min \\{ a_{i},b_{i}\\}  $ si es un kernel válido y es conocida como **Histogram Intersection Kernel** y es muy usado en problemas de clasificación eficientemente en SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 .(Reto) Sea $f:[-1, 1]^{n} \\rightarrow [-1, 1]$ una función $\\rho$-Lipschitz. Para algún $\\epsilon > 0$, construye una red neuronal $f:[-1, 1]^{n} \\rightarrow [-1, 1]$ con la función de activación sigmoidea, tal que por cada $x \\in [-1, 1]^{n}$, se cumple que $\\vert f(x) - N(x)\\vert \\leq \\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tu respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las referencias a los ejercicios son :\n",
    "\n",
    "   * David Barber, Bayesian Reasoning and Machine Learning, Cambridge University Press, 2017.\n",
    "   * Pattern Recognition and Machine Learning de Chris Bishop 2006.\n",
    "   * Shai Shalev-Shwartz, and Shai Ben-David,Understanding Machine Learning: From Theory to Algorithms Cambridge University Press, 2014. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
